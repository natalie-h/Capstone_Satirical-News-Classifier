{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After speaking with David Yerrington on 6/19, I will pursue the following for my capstone project:*\n",
    "\n",
    "Creating a supervised classifier using a variety of machine learning models that will identify whether or not a news article in English from the internet is satirical or not. Living in an age of often-misleading news, this is a highly relevant project that, with the aid of topic modeling, may help in identifying what cues lead to this.\n",
    "\n",
    "*Timeline:*\n",
    "\n",
    "Ideally, data collection for the training set will be done by June 27th; validation data will be collected from June 27th-July 6th due to the nature of webscraping. EDA, pre-processing, and modeling should take place by the 6th to ensure adequate time to change anything necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection, Methods & Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Collection:*\n",
    "   \n",
    "To collect this data, I will be using the Newspaper3k library (http://newspaper.readthedocs.io/en/latest/) for article scraping. (Please see note below regarding revisions to original idea for limitations imposed on this.) I will scrape from a variety of nationally-recognized official (read: non-satirical) and satirical newspaper websites. \n",
    "\n",
    "To accommodate for political-leaning, I will take data from various non-satirical publications per this website:\n",
    "http://www.businessinsider.com/what-your-preferred-news-outlet-says-about-your-political-ideology-2014-10\n",
    "\n",
    "In addition, I have some data already collected from the New York Times available from this Kaggle dataset:\n",
    "https://www.kaggle.com/aashita/nyt-comments\n",
    "\n",
    "As for the satirical newspapers, I will scrape what is available from the following websites:\n",
    "https://en.wikipedia.org/wiki/List_of_satirical_news_websites\n",
    "\n",
    "For both classes, I will aim to collect at least 2-5k articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pre-processing & Methods:*\n",
    "\n",
    "In exploring the data, I will begin with simple text vectorization (without stop words initially, per suggestion by David Yerrington). Once complete, I'll locate the most common tokens (approximately 100) that appear across the articles. With this, creating a bar graph and topic modeling will be possible to find overlap.\n",
    "\n",
    "Once complete, I will use a variety of n-gram types to further investigate context. These will include bi-grams, trim-grams, and quad-grams.\n",
    "\n",
    "Following this, pre-processing like TF-IDF and lemmatizing/stemming will be performed. Any significant n-gram ranges from above will be incorporated to determine how much of an influence they may have on a model.\n",
    "\n",
    "An additional method that may be considered will be a sensitivity analysis. Using TextBlob (http://textblob.readthedocs.io/en/dev/) will allow for a lot of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Modeling:*\n",
    "\n",
    "The intention will be to begin with a few machine learning models, adding in complexity with each version. A few of the intended models will be Logistic Regression, SVM, and Na√Øve Bayes. Once these have shown a reliable precision rate, I will build a Neural Network (likely Convolutional) that uses the features identified above and compares this precision to the benchmark figures from the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risks & Assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following are being considered:*\n",
    "\n",
    "- All articles will be in American English. In the event that more data is needed, this may be extended to British English.\n",
    "- Article lengths will be controlled for - that is, a maximum word count per article will be determined.\n",
    "- All articles will have a publication date of after November 9, 2016. Depending on what is found, this may be revised to January 20, 2017 (Presidential Inauguration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals & Success Criteria Revisited**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier will be distinguished from previous attempts in that it will exclusively on articles following the US federal elections on November 9, 2016. That is, this classifier will need to excel at picking out satire from irony (or overt racism/sexism/xenophobia/elitism).\n",
    "\n",
    "Additionally, taking into consideration the work that has been done on this type of classifier in the past, the aim of this classifier will not only be accuracy, but to deploy methods that weren't readily available at the time of publication of this article:\n",
    "https://www.researchgate.net/profile/Victoria_Rubin/publication/301650504_Fake_News_or_Truth_Using_Satirical_Cues_to_Detect_Potentially_Misleading_News/links/571fed4c08aeaced788acd8e/Fake-News-or-Truth-Using-Satirical-Cues-to-Detect-Potentially-Misleading-News.pdf\n",
    "\n",
    "Another aspect, depending on how the model performs, will be to examine if the same model can be applied to the headlines of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please see file 'Preliminary_EDA.ipynb'*\n",
    "\n",
    "After preliminary EDA of the NYT articles downloaded from the Kaggle dataset above, a few things are clear:\n",
    "\n",
    "1. Article word length varies **substantially**. (See histogram.)\n",
    "2. There are a few columns that divide the articles up by content type. I will leave these intact until the other news sites have been scraped to have a better idea of where they overlap in subjects. This will be especially important in determining the limitations of the satirical articles.\n",
    "- Along these lines, it will be paramount to determine if Op-eds and articles such as Letters/Blog will be worth keeping.\n",
    "3. Fortunately, the NYT content itself requires very minimal cleaning. However, will need to write a function that cuts off the articles at their maximum permitted word counts.\n",
    "4. Will need to incorporate a new column to the dataframe identifying class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
